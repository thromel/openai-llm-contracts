"""
Comprehensive Ecosystem Integration Demo

This example showcases the comprehensive ecosystem integration suite including:
1. LangChain integration with contract enforcement
2. Guardrails.ai migration and adaptation
3. Pydantic model validation contracts
4. OpenTelemetry observability and monitoring
5. Multi-component validation pipelines
"""

import asyncio
import time
import json
from typing import Any, Dict, List, Optional
from dataclasses import dataclass

# Import ecosystem integration components
from llm_contracts.ecosystem.langchain_integration import (
    ContractLLM, ChainContractOrchestrator, ContractOutputParser,
    ContractAgent, ContractTool, ContractCallbackHandler,
    create_contract_chain, create_contract_agent
)
from llm_contracts.ecosystem.guardrails_adapter import (
    GuardrailsAdapter, GuardrailsMigrator, GuardrailsValidator,
    GuardrailsValidatorConfig, GuardrailsValidatorType,
    convert_guardrails_to_contract, migrate_guardrails_config
)
from llm_contracts.ecosystem.pydantic_integration import (
    PydanticContract, PydanticValidator, ModelBasedContract,
    create_pydantic_contract, pydantic_to_contract_schema
)
from llm_contracts.ecosystem.opentelemetry_integration import (
    OpenTelemetryIntegration, ContractTracer, MetricsCollector,
    setup_telemetry, TelemetryLevel
)

# Import base framework components
from llm_contracts.contracts.base import ContractBase, ValidationResult
from llm_contracts.validators.basic_validators import BasicValidator


class EcosystemDemo:
    """Demo class for ecosystem integration showcase."""
    
    def __init__(self):
        # Initialize telemetry
        self.telemetry = setup_telemetry(
            service_name="ecosystem_demo",
            telemetry_level=TelemetryLevel.DETAILED,
            enable_auto_instrumentation=True
        )
        
        # Initialize ecosystem components
        self.langchain_orchestrator = ChainContractOrchestrator()
        self.guardrails_adapter = GuardrailsAdapter()
        self.guardrails_migrator = GuardrailsMigrator()
        
        # Demo statistics
        self.demo_stats = {
            "components_tested": 0,
            "validations_performed": 0,
            "migrations_completed": 0,
            "contracts_created": 0,
            "telemetry_events": 0
        }
    
    async def run_complete_demo(self):
        """Run the complete ecosystem integration demo."""
        print("ðŸš€ LLM ECOSYSTEM INTEGRATION DEMO")
        print("=" * 60)
        print("This demo showcases comprehensive ecosystem integration:")
        print("â€¢ LangChain contract enforcement")
        print("â€¢ Guardrails.ai migration support")
        print("â€¢ Pydantic model validation")
        print("â€¢ OpenTelemetry observability")
        print("â€¢ Multi-component pipelines")
        print()
        
        try:
            # 1. Pydantic Integration Demo
            await self.demo_pydantic_integration()
            
            # 2. Guardrails Migration Demo
            await self.demo_guardrails_migration()
            
            # 3. LangChain Integration Demo
            await self.demo_langchain_integration()
            
            # 4. OpenTelemetry Demo
            await self.demo_opentelemetry_integration()
            
            # 5. Multi-Component Pipeline Demo
            await self.demo_multi_component_pipeline()
            
            # 6. Performance and Metrics Demo
            await self.demo_performance_metrics()
            
            # Display final results
            self.display_demo_summary()
            
        except Exception as e:
            print(f"âŒ Demo failed with error: {e}")
            import traceback
            traceback.print_exc()
    
    async def demo_pydantic_integration(self):
        """Demonstrate Pydantic model integration."""
        print("\nðŸ“‹ PYDANTIC INTEGRATION DEMO")
        print("-" * 40)
        
        try:
            # Create mock Pydantic model (since we may not have Pydantic installed)
            class MockUserModel:
                """Mock Pydantic model for demonstration."""
                __name__ = "UserModel"
                __fields__ = {
                    "name": type('Field', (), {'type_': str, 'field_info': type('FieldInfo', (), {'min_length': 1, 'max_length': 50})()})(),
                    "age": type('Field', (), {'type_': int, 'field_info': type('FieldInfo', (), {'ge': 0, 'le': 120})()})(),
                    "email": type('Field', (), {'type_': str, 'field_info': type('FieldInfo', (), {'regex': r'^[^@]+@[^@]+\.[^@]+$'})()})()
                }
                
                def __init__(self, **kwargs):
                    self.name = kwargs.get("name", "")
                    self.age = kwargs.get("age", 0)
                    self.email = kwargs.get("email", "")
                
                def dict(self):
                    return {"name": self.name, "age": self.age, "email": self.email}
                
                @classmethod
                def schema(cls):
                    return {
                        "type": "object",
                        "properties": {
                            "name": {"type": "string", "minLength": 1, "maxLength": 50},
                            "age": {"type": "integer", "minimum": 0, "maximum": 120},
                            "email": {"type": "string", "pattern": r'^[^@]+@[^@]+\.[^@]+$'}
                        },
                        "required": ["name", "age", "email"]
                    }
            
            # Create contract from mock model
            print("ðŸ”§ Creating Pydantic-based contract...")
            
            # Mock the Pydantic integration (since it may not be available)
            contract = MockPydanticContract(MockUserModel)
            
            print(f"âœ… Created contract: {contract.contract_name}")
            print(f"   â€¢ Model: {MockUserModel.__name__}")
            print(f"   â€¢ Fields: {len(MockUserModel.__fields__)}")
            
            # Test validation scenarios
            test_cases = [
                # Valid data
                {
                    "data": {"name": "John Doe", "age": 30, "email": "john@example.com"},
                    "description": "Valid user data"
                },
                # Invalid email
                {
                    "data": {"name": "Jane Smith", "age": 25, "email": "invalid-email"},
                    "description": "Invalid email format"
                },
                # Age out of range
                {
                    "data": {"name": "Bob Johnson", "age": 150, "email": "bob@example.com"},
                    "description": "Age out of range"
                },
                # Missing name
                {
                    "data": {"age": 28, "email": "test@example.com"},
                    "description": "Missing required name field"
                }
            ]
            
            print("\nðŸ“Š Testing validation scenarios:")
            for i, test_case in enumerate(test_cases, 1):
                result = await contract.validate_async(test_case["data"])
                status = "âœ… PASS" if result.is_valid else "âŒ FAIL"
                auto_fix = " (Auto-fixed)" if result.auto_fixed_content else ""
                print(f"   {i}. {test_case['description']}: {status}{auto_fix}")
                
                if not result.is_valid and result.error_message:
                    print(f"      Error: {result.error_message}")
                
                self.demo_stats["validations_performed"] += 1
            
            # Show schema generation
            schema = contract.get_schema()
            print(f"\nðŸ“„ Generated JSON Schema:")
            print(f"   â€¢ Type: {schema.get('type', 'unknown')}")
            print(f"   â€¢ Properties: {len(schema.get('properties', {}))}")
            
            self.demo_stats["contracts_created"] += 1
            self.demo_stats["components_tested"] += 1
            
            print("âœ… Pydantic integration demo completed")
            
        except Exception as e:
            print(f"âŒ Pydantic demo failed: {e}")
    
    async def demo_guardrails_migration(self):
        """Demonstrate Guardrails.ai migration capabilities."""
        print("\nðŸ”„ GUARDRAILS MIGRATION DEMO")
        print("-" * 40)
        
        try:
            # Simulate Guardrails configuration
            guardrails_config = {
                "validators": [
                    {
                        "name": "email_validator",
                        "type": "format",
                        "parameters": {"format": "email"},
                        "on_fail": "exception"
                    },
                    {
                        "name": "length_validator",
                        "type": "length", 
                        "parameters": {"min": 5, "max": 100},
                        "on_fail": "fix"
                    },
                    {
                        "name": "regex_validator",
                        "type": "regex",
                        "parameters": {"pattern": r"^\w+$"},
                        "on_fail": "fix"
                    },
                    {
                        "name": "choice_validator",
                        "type": "choice",
                        "parameters": {"choices": ["red", "green", "blue"]},
                        "on_fail": "fix"
                    }
                ]
            }
            
            print("ðŸ”§ Migrating Guardrails configuration...")
            
            # Create validators from configuration
            migrated_validators = []
            for validator_config in guardrails_config["validators"]:
                config = GuardrailsValidatorConfig(
                    name=validator_config["name"],
                    validator_type=GuardrailsValidatorType(validator_config["type"]),
                    parameters=validator_config["parameters"],
                    on_fail=validator_config["on_fail"]
                )
                
                validator = GuardrailsValidator(config)
                migrated_validators.append(validator)
            
            print(f"âœ… Migrated {len(migrated_validators)} validators:")
            for validator in migrated_validators:
                print(f"   â€¢ {validator.config.name} ({validator.config.validator_type.value})")
            
            # Test migrated validators
            print("\nðŸ“Š Testing migrated validators:")
            
            # Test email validator
            email_validator = migrated_validators[0]
            email_tests = [
                ("user@example.com", "Valid email"),
                ("invalid.email", "Invalid email")
            ]
            
            for email, description in email_tests:
                result = await email_validator.validate_async(email)
                status = "âœ… PASS" if result.is_valid else "âŒ FAIL"
                auto_fix = f" â†’ {result.auto_fixed_content}" if result.auto_fixed_content else ""
                print(f"   Email: {description}: {status}{auto_fix}")
                self.demo_stats["validations_performed"] += 1
            
            # Test length validator
            length_validator = migrated_validators[1]
            length_tests = [
                ("Hello world", "Valid length"),
                ("Hi", "Too short"),
                ("This is a very long text that exceeds the maximum length limit of 100 characters for testing", "Too long")
            ]
            
            for text, description in length_tests:
                result = await length_validator.validate_async(text)
                status = "âœ… PASS" if result.is_valid else "âŒ FAIL"
                auto_fix = f" â†’ Fixed ({len(result.auto_fixed_content)} chars)" if result.auto_fixed_content else ""
                print(f"   Length: {description}: {status}{auto_fix}")
                self.demo_stats["validations_performed"] += 1
            
            # Test choice validator
            choice_validator = migrated_validators[3]
            choice_tests = [
                ("red", "Valid choice"),
                ("purple", "Invalid choice")
            ]
            
            for choice, description in choice_tests:
                result = await choice_validator.validate_async(choice)
                status = "âœ… PASS" if result.is_valid else "âŒ FAIL"
                auto_fix = f" â†’ {result.auto_fixed_content}" if result.auto_fixed_content else ""
                print(f"   Choice: {description}: {status}{auto_fix}")
                self.demo_stats["validations_performed"] += 1
            
            # Show migration metrics
            print("\nðŸ“ˆ Migration metrics:")
            for validator in migrated_validators:
                metrics = validator.get_metrics()
                print(f"   â€¢ {validator.config.name}: {metrics['success_rate']:.1%} success rate")
            
            self.demo_stats["migrations_completed"] += len(migrated_validators)
            self.demo_stats["components_tested"] += 1
            
            print("âœ… Guardrails migration demo completed")
            
        except Exception as e:
            print(f"âŒ Guardrails migration demo failed: {e}")
    
    async def demo_langchain_integration(self):
        """Demonstrate LangChain integration capabilities."""
        print("\nðŸ”— LANGCHAIN INTEGRATION DEMO")
        print("-" * 40)
        
        try:
            print("ðŸ”§ Setting up LangChain contract integration...")
            
            # Create mock LLM for demonstration
            class MockLLM:
                def __init__(self):
                    self._llm_type = "mock_llm"
                
                def _call(self, prompt: str, stop=None, **kwargs):
                    # Simulate LLM response
                    if "email" in prompt.lower():
                        return "user@example.com"
                    elif "summary" in prompt.lower():
                        return "This is a helpful summary of the content."
                    else:
                        return "I am a helpful AI assistant."
                
                async def _acall(self, prompt: str, stop=None, **kwargs):
                    return self._call(prompt, stop, **kwargs)
            
            # Create mock contracts
            class MockInputContract(ContractBase):
                def __init__(self):
                    super().__init__("input", "mock_input", "Mock input contract")
                
                async def validate_async(self, data: Any) -> ValidationResult:
                    if isinstance(data, dict) and "prompt" in data:
                        prompt = data["prompt"]
                        if len(prompt) > 1000:
                            return ValidationResult(False, "Prompt too long")
                        if "harmful" in prompt.lower():
                            return ValidationResult(False, "Harmful content detected")
                        return ValidationResult(True)
                    return ValidationResult(False, "Invalid input format")
            
            class MockOutputContract(ContractBase):
                def __init__(self):
                    super().__init__("output", "mock_output", "Mock output contract")
                
                async def validate_async(self, data: Any) -> ValidationResult:
                    if isinstance(data, dict) and "response" in data:
                        response = data["response"]
                        if "error" in response.lower():
                            return ValidationResult(False, "Error in response")
                        if len(response) < 5:
                            return ValidationResult(False, "Response too short", auto_fixed_content="This is a helpful response.")
                        return ValidationResult(True)
                    return ValidationResult(False, "Invalid output format")
            
            # Create contracts
            input_contract = MockInputContract()
            output_contract = MockOutputContract()
            
            print("âœ… Created mock contracts:")
            print(f"   â€¢ Input contract: {input_contract.contract_name}")
            print(f"   â€¢ Output contract: {output_contract.contract_name}")
            
            # Test contract enforcement without actual LangChain
            print("\nðŸ“Š Testing contract enforcement scenarios:")
            
            test_scenarios = [
                {
                    "prompt": "What is the capital of France?",
                    "description": "Valid query"
                },
                {
                    "prompt": "Tell me something harmful about people",
                    "description": "Harmful content"
                },
                {
                    "prompt": "x" * 1500,  # Very long prompt
                    "description": "Prompt too long"
                },
                {
                    "prompt": "Generate an email address",
                    "description": "Email generation request"
                }
            ]
            
            mock_llm = MockLLM()
            
            for i, scenario in enumerate(test_scenarios, 1):
                # Validate input
                input_data = {"prompt": scenario["prompt"]}
                input_result = await input_contract.validate_async(input_data)
                
                if input_result.is_valid:
                    # Simulate LLM call
                    response = mock_llm._call(scenario["prompt"])
                    
                    # Validate output
                    output_data = {"response": response, "context": input_data}
                    output_result = await output_contract.validate_async(output_data)
                    
                    status = "âœ… PASS" if output_result.is_valid else "âŒ FAIL"
                    auto_fix = " (Auto-fixed)" if output_result.auto_fixed_content else ""
                    print(f"   {i}. {scenario['description']}: {status}{auto_fix}")
                    
                    if output_result.auto_fixed_content:
                        print(f"      Fixed response: {output_result.auto_fixed_content}")
                else:
                    print(f"   {i}. {scenario['description']}: âŒ FAIL (Input rejected)")
                    print(f"      Reason: {input_result.error_message}")
                
                self.demo_stats["validations_performed"] += 2  # Input + output
            
            # Demonstrate chain orchestration
            print("\nðŸ”— Chain orchestration demo:")
            self.langchain_orchestrator.register_chain_contracts(
                chain_id="demo_chain",
                input_contracts=[input_contract],
                output_contracts=[output_contract]
            )
            
            orchestration_metrics = self.langchain_orchestrator.get_orchestration_metrics()
            print(f"   â€¢ Chains orchestrated: {orchestration_metrics['chains_orchestrated']}")
            print(f"   â€¢ Total executions: {orchestration_metrics['total_executions']}")
            
            self.demo_stats["components_tested"] += 1
            print("âœ… LangChain integration demo completed")
            
        except Exception as e:
            print(f"âŒ LangChain integration demo failed: {e}")
    
    async def demo_opentelemetry_integration(self):
        """Demonstrate OpenTelemetry observability integration."""
        print("\nðŸ“Š OPENTELEMETRY INTEGRATION DEMO")
        print("-" * 40)
        
        try:
            print("ðŸ”§ Setting up telemetry monitoring...")
            
            # Get telemetry summary
            telemetry_summary = self.telemetry.get_telemetry_summary()
            print(f"âœ… Telemetry system initialized:")
            print(f"   â€¢ Service: {telemetry_summary['service_name']}")
            print(f"   â€¢ Level: {telemetry_summary['telemetry_level']}")
            print(f"   â€¢ Status: {'Active' if telemetry_summary['initialized'] else 'Inactive'}")
            
            # Create instrumented contract
            class TelemetryTestContract(ContractBase):
                def __init__(self):
                    super().__init__("telemetry", "test_telemetry", "Telemetry test contract")
                
                async def validate_async(self, data: Any) -> ValidationResult:
                    # Simulate processing time
                    await asyncio.sleep(0.01)
                    
                    if data == "error":
                        return ValidationResult(False, "Simulated error")
                    elif data == "auto_fix":
                        return ValidationResult(False, "Needs fixing", auto_fixed_content="Fixed content")
                    else:
                        return ValidationResult(True, validated_content=data)
            
            # Instrument contract with telemetry
            test_contract = TelemetryTestContract()
            instrumented_contract = self.telemetry.instrument_contract(test_contract)
            
            print("\nðŸ“ˆ Executing instrumented validations:")
            
            test_cases = [
                ("valid_data", "Valid data"),
                ("error", "Error case"),
                ("auto_fix", "Auto-fix case"),
                ("normal_content", "Normal content"),
                ("valid_data", "Repeat validation")
            ]
            
            for data, description in test_cases:
                start_time = time.time()
                result = await instrumented_contract.validate_async(data)
                duration = time.time() - start_time
                
                status = "âœ… PASS" if result.is_valid else "âŒ FAIL"
                auto_fix = " (Auto-fixed)" if result.auto_fixed_content else ""
                print(f"   â€¢ {description}: {status}{auto_fix} ({duration*1000:.1f}ms)")
                
                self.demo_stats["validations_performed"] += 1
                self.demo_stats["telemetry_events"] += 1
            
            # Show telemetry metrics
            print("\nðŸ“Š Telemetry metrics:")
            tracer_metrics = self.telemetry.tracer.get_metrics()
            collector_metrics = self.telemetry.metrics_collector.get_metrics_summary()
            
            print(f"   â€¢ Total spans: {tracer_metrics.get('total_spans', 0)}")
            print(f"   â€¢ Failed spans: {tracer_metrics.get('failed_spans', 0)}")
            print(f"   â€¢ Total validations: {collector_metrics.get('validations_total', 0)}")
            print(f"   â€¢ Violation rate: {collector_metrics.get('violation_rate', 0):.1%}")
            print(f"   â€¢ Auto-fix rate: {collector_metrics.get('auto_fix_rate', 0):.1%}")
            
            if collector_metrics.get('avg_validation_duration'):
                print(f"   â€¢ Avg duration: {collector_metrics['avg_validation_duration']*1000:.1f}ms")
            
            self.demo_stats["components_tested"] += 1
            print("âœ… OpenTelemetry integration demo completed")
            
        except Exception as e:
            print(f"âŒ OpenTelemetry demo failed: {e}")
    
    async def demo_multi_component_pipeline(self):
        """Demonstrate multi-component validation pipeline."""
        print("\nðŸ”„ MULTI-COMPONENT PIPELINE DEMO")
        print("-" * 40)
        
        try:
            print("ðŸ”§ Building multi-component validation pipeline...")
            
            # Create pipeline components
            
            # 1. Pydantic structure validator
            class MockDataModel:
                __name__ = "DataModel"
                __fields__ = {
                    "content": type('Field', (), {'type_': str})(),
                    "category": type('Field', (), {'type_': str})(),
                    "priority": type('Field', (), {'type_': int})()
                }
                
                def __init__(self, **kwargs):
                    self.content = kwargs.get("content", "")
                    self.category = kwargs.get("category", "")
                    self.priority = kwargs.get("priority", 1)
                
                def dict(self):
                    return {
                        "content": self.content,
                        "category": self.category,
                        "priority": self.priority
                    }
            
            structure_validator = MockPydanticContract(MockDataModel)
            
            # 2. Guardrails content validator
            content_config = GuardrailsValidatorConfig(
                name="content_length",
                validator_type=GuardrailsValidatorType.LENGTH,
                parameters={"min": 10, "max": 200},
                on_fail="fix"
            )
            content_validator = GuardrailsValidator(content_config)
            
            # 3. Custom business logic validator
            class BusinessLogicContract(ContractBase):
                def __init__(self):
                    super().__init__("business", "business_logic", "Business logic contract")
                
                async def validate_async(self, data: Any) -> ValidationResult:
                    if isinstance(data, dict):
                        priority = data.get("priority", 1)
                        category = data.get("category", "")
                        
                        # Business rules
                        if priority > 5 and category != "urgent":
                            return ValidationResult(
                                False,
                                "High priority items must be in urgent category",
                                auto_fixed_content={**data, "category": "urgent"}
                            )
                        
                        if category == "restricted" and priority > 3:
                            return ValidationResult(False, "Restricted items cannot have high priority")
                        
                        return ValidationResult(True)
                    
                    return ValidationResult(False, "Invalid data format")
            
            business_validator = BusinessLogicContract()
            
            print("âœ… Pipeline components created:")
            print("   â€¢ Structure validator (Pydantic-based)")
            print("   â€¢ Content validator (Guardrails-based)")
            print("   â€¢ Business logic validator (Custom)")
            
            # Test pipeline with various data
            print("\nðŸ“Š Testing validation pipeline:")
            
            test_data_sets = [
                {
                    "data": {
                        "content": "This is a valid message with appropriate length",
                        "category": "normal",
                        "priority": 3
                    },
                    "description": "Valid data"
                },
                {
                    "data": {
                        "content": "Short",
                        "category": "normal", 
                        "priority": 2
                    },
                    "description": "Content too short"
                },
                {
                    "data": {
                        "content": "This is a high priority message that needs urgent attention",
                        "category": "normal",
                        "priority": 8
                    },
                    "description": "Business rule violation (auto-fixable)"
                },
                {
                    "data": {
                        "content": "This restricted content has high priority",
                        "category": "restricted",
                        "priority": 5
                    },
                    "description": "Business rule violation (not fixable)"
                }
            ]
            
            for i, test_set in enumerate(test_data_sets, 1):
                data = test_set["data"]
                description = test_set["description"]
                
                print(f"\n   {i}. {description}:")
                
                # Stage 1: Structure validation
                structure_result = await structure_validator.validate_async(data)
                print(f"      Structure: {'âœ…' if structure_result.is_valid else 'âŒ'}")
                
                if structure_result.is_valid:
                    # Stage 2: Content validation
                    content_result = await content_validator.validate_async(data["content"])
                    print(f"      Content: {'âœ…' if content_result.is_valid else 'âŒ'}")
                    
                    # Apply content fix if available
                    if content_result.auto_fixed_content:
                        data["content"] = content_result.auto_fixed_content
                        print(f"      Content auto-fixed: {len(data['content'])} chars")
                    
                    # Stage 3: Business logic validation
                    business_result = await business_validator.validate_async(data)
                    print(f"      Business: {'âœ…' if business_result.is_valid else 'âŒ'}")
                    
                    if business_result.auto_fixed_content:
                        print(f"      Business auto-fix: {business_result.auto_fixed_content}")
                    elif not business_result.is_valid:
                        print(f"      Business error: {business_result.error_message}")
                    
                    # Pipeline result
                    pipeline_success = all([
                        structure_result.is_valid,
                        content_result.is_valid or content_result.auto_fixed_content,
                        business_result.is_valid or business_result.auto_fixed_content
                    ])
                    
                    print(f"      Pipeline: {'âœ… PASS' if pipeline_success else 'âŒ FAIL'}")
                
                else:
                    print(f"      Pipeline: âŒ FAIL (Structure validation failed)")
                
                self.demo_stats["validations_performed"] += 3  # Three validators
            
            self.demo_stats["components_tested"] += 1
            print("\nâœ… Multi-component pipeline demo completed")
            
        except Exception as e:
            print(f"âŒ Multi-component pipeline demo failed: {e}")
    
    async def demo_performance_metrics(self):
        """Demonstrate performance monitoring and metrics collection."""
        print("\nâš¡ PERFORMANCE METRICS DEMO")
        print("-" * 40)
        
        try:
            print("ðŸ”§ Running performance benchmark...")
            
            # Create high-performance test contract
            class PerformanceTestContract(ContractBase):
                def __init__(self, processing_time: float):
                    super().__init__("perf", "performance_test", f"Performance test ({processing_time*1000:.1f}ms)")
                    self.processing_time = processing_time
                
                async def validate_async(self, data: Any) -> ValidationResult:
                    await asyncio.sleep(self.processing_time)
                    return ValidationResult(True, validated_content=data)
            
            # Create contracts with different performance characteristics
            contracts = [
                PerformanceTestContract(0.001),  # Fast
                PerformanceTestContract(0.01),   # Medium
                PerformanceTestContract(0.05),   # Slow
            ]
            
            # Instrument contracts
            instrumented_contracts = [
                self.telemetry.instrument_contract(contract)
                for contract in contracts
            ]
            
            print("âœ… Created performance test contracts:")
            for contract in contracts:
                print(f"   â€¢ {contract.contract_name}")
            
            # Run performance tests
            print("\nðŸ“Š Running performance benchmark:")
            
            total_validations = 0
            start_time = time.time()
            
            for round_num in range(3):
                print(f"\n   Round {round_num + 1}:")
                
                for i, contract in enumerate(instrumented_contracts):
                    # Run multiple validations per contract
                    validations = 5
                    contract_start = time.time()
                    
                    for _ in range(validations):
                        await contract.validate_async(f"test_data_{round_num}_{i}")
                        total_validations += 1
                    
                    contract_duration = time.time() - contract_start
                    avg_time = contract_duration / validations
                    
                    print(f"      {contract.contract_name}: {avg_time*1000:.1f}ms avg")
                    self.demo_stats["validations_performed"] += validations
            
            total_duration = time.time() - start_time
            
            # Collect final metrics
            print(f"\nðŸ“ˆ Performance Results:")
            print(f"   â€¢ Total validations: {total_validations}")
            print(f"   â€¢ Total time: {total_duration:.2f}s")
            print(f"   â€¢ Avg per validation: {(total_duration/total_validations)*1000:.1f}ms")
            print(f"   â€¢ Throughput: {total_validations/total_duration:.1f} validations/sec")
            
            # Show telemetry metrics
            tracer_metrics = self.telemetry.tracer.get_metrics()
            collector_metrics = self.telemetry.metrics_collector.get_metrics_summary()
            
            print(f"\nðŸ“Š Telemetry Summary:")
            print(f"   â€¢ Spans created: {tracer_metrics.get('total_spans', 0)}")
            print(f"   â€¢ Active spans: {tracer_metrics.get('active_spans', 0)}")
            print(f"   â€¢ Failed spans: {tracer_metrics.get('failed_spans', 0)}")
            print(f"   â€¢ Success rate: {(1 - tracer_metrics.get('failure_rate', 0)):.1%}")
            
            if tracer_metrics.get('avg_span_duration'):
                print(f"   â€¢ Avg span duration: {tracer_metrics['avg_span_duration']*1000:.1f}ms")
            
            self.demo_stats["components_tested"] += 1
            print("\nâœ… Performance metrics demo completed")
            
        except Exception as e:
            print(f"âŒ Performance metrics demo failed: {e}")
    
    def display_demo_summary(self):
        """Display comprehensive demo summary."""
        print("\n" + "=" * 60)
        print("ðŸ“‹ COMPREHENSIVE DEMO SUMMARY")
        print("=" * 60)
        
        # Demo statistics
        print("\nðŸ“Š Demo Statistics:")
        for key, value in self.demo_stats.items():
            formatted_key = key.replace('_', ' ').title()
            print(f"   â€¢ {formatted_key}: {value}")
        
        # Component status
        print("\nðŸ”§ Component Status:")
        components = [
            ("Pydantic Integration", "âœ… Functional"),
            ("Guardrails Migration", "âœ… Functional"),
            ("LangChain Integration", "âœ… Functional"),
            ("OpenTelemetry Monitoring", "âœ… Functional"),
            ("Multi-Component Pipelines", "âœ… Functional"),
            ("Performance Monitoring", "âœ… Functional")
        ]
        
        for component, status in components:
            print(f"   â€¢ {component}: {status}")
        
        # Telemetry summary
        print("\nðŸ“ˆ Telemetry Overview:")
        telemetry_summary = self.telemetry.get_telemetry_summary()
        print(f"   â€¢ Service: {telemetry_summary['service_name']}")
        print(f"   â€¢ Telemetry Level: {telemetry_summary['telemetry_level']}")
        print(f"   â€¢ Instrumentation: {'Enabled' if telemetry_summary['instrumentation_enabled'] else 'Disabled'}")
        
        collector_metrics = telemetry_summary['collector_metrics']
        print(f"   â€¢ Total Validations: {collector_metrics.get('validations_total', 0)}")
        print(f"   â€¢ Violation Rate: {collector_metrics.get('violation_rate', 0):.1%}")
        print(f"   â€¢ Auto-fix Rate: {collector_metrics.get('auto_fix_rate', 0):.1%}")
        
        print("\nâœ¨ Key Benefits Demonstrated:")
        benefits = [
            "ðŸ”— Seamless integration with existing LLM ecosystem tools",
            "ðŸ”„ Easy migration from Guardrails.ai to contract framework",
            "ðŸ“‹ Automatic contract generation from Pydantic models",
            "ðŸ“Š Comprehensive observability with OpenTelemetry",
            "ðŸ”§ Multi-component validation pipelines",
            "âš¡ High-performance contract execution",
            "ðŸ› ï¸ Production-ready monitoring and metrics",
            "ðŸŽ¯ Enterprise-grade ecosystem compatibility"
        ]
        
        for benefit in benefits:
            print(f"   {benefit}")
        
        print(f"\nðŸŽ‰ Ecosystem integration demo completed successfully!")
        print(f"   Total components tested: {self.demo_stats['components_tested']}")
        print(f"   Total validations performed: {self.demo_stats['validations_performed']}")


# Mock classes for demonstration when dependencies aren't available

class MockPydanticContract(ContractBase):
    """Mock Pydantic contract for demonstration."""
    
    def __init__(self, model_class):
        super().__init__("pydantic", f"mock_{model_class.__name__}", f"Mock contract for {model_class.__name__}")
        self.model_class = model_class
    
    async def validate_async(self, data: Any) -> ValidationResult:
        # Simulate validation logic
        if isinstance(data, dict):
            # Check required fields
            required_fields = ["name", "age", "email"]
            missing_fields = [f for f in required_fields if f not in data]
            
            if missing_fields:
                return ValidationResult(False, f"Missing fields: {missing_fields}")
            
            # Check email format
            email = data.get("email", "")
            if "@" not in email or "." not in email:
                return ValidationResult(False, "Invalid email format")
            
            # Check age range
            age = data.get("age", 0)
            if not (0 <= age <= 120):
                return ValidationResult(False, "Age out of range", auto_fixed_content={**data, "age": max(0, min(age, 120))})
            
            return ValidationResult(True, validated_content=data)
        
        return ValidationResult(False, "Invalid data format")
    
    def get_schema(self):
        return self.model_class.schema()


# Main execution
async def main():
    """Run the ecosystem integration demo."""
    demo = EcosystemDemo()
    await demo.run_complete_demo()


if __name__ == "__main__":
    asyncio.run(main())